{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11864f6f-468a-4c6c-abc9-21efb462cab1",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db1856-ce14-4e74-b392-b1de0b7db961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "train_values = pd.read_csv(\"/kaggle/input/earthquake-survey/train_values.csv\")\n",
    "train_labels = pd.read_csv(\"/kaggle/input/earthquake-survey/train_labels.csv\")\n",
    "\n",
    "# Drop 'building_id' (not needed for training)\n",
    "train_values.drop(columns=['building_id'], inplace=True)\n",
    "train_labels.drop(columns=['building_id'], inplace=True)\n",
    "\n",
    "# Encode target variable (damage_grade)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels['damage_grade'] = label_encoder.fit_transform(train_labels['damage_grade'])\n",
    "\n",
    "# Convert categorical columns to strings\n",
    "categorical_features = ['land_surface_condition', 'foundation_type', 'roof_type', \n",
    "                        'ground_floor_type', 'other_floor_type', 'position', \n",
    "                        'plan_configuration', 'legal_ownership_status']\n",
    "train_values[categorical_features] = train_values[categorical_features].astype(str)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_values = pd.get_dummies(train_values, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = train_values.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_values[numerical_features] = scaler.fit_transform(train_values[numerical_features])\n",
    "\n",
    "# Convert train_labels to numeric\n",
    "y = train_labels['damage_grade'].astype(np.int64)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_values, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train, X_val = X_train.to_numpy(), X_val.to_numpy()\n",
    "X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "y_train, y_val = y_train.astype(np.int64), y_val.astype(np.int64)\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "# Define the CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_cnn_model((X_train.shape[1], 1))\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "checkpoint = callbacks.ModelCheckpoint(\"best_model_cnn.keras\", monitor='val_accuracy', save_best_only=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                    epochs=100, batch_size=256, \n",
    "                    callbacks=[early_stopping, checkpoint, reduce_lr], verbose=1)\n",
    "\n",
    "# Save training history plot\n",
    "def save_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "save_training_history(history)\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions = np.argmax(model.predict(X_val), axis=1)\n",
    "cm = confusion_matrix(y_val, predictions)\n",
    "pd.DataFrame(cm).to_csv('confusion_matrix.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, predictions, target_names=label_encoder.classes_.astype(str), output_dict=True)\n",
    "pd.DataFrame(report).T.to_csv('CNN_model_result.csv', index=True)\n",
    "\n",
    "print(\"Results saved: training_history.png, confusion_matrix.csv, CNN_model_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358922c0-1a57-4e67-88c0-9605f31405c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb1e23a-b43b-4465-81f9-68e831f134e2",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85865dec-5a8a-4acc-85a4-e7243ff86fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "train_values = pd.read_csv(\"/kaggle/input/earthquake-survey/train_values.csv\")\n",
    "train_labels = pd.read_csv(\"/kaggle/input/earthquake-survey/train_labels.csv\")\n",
    "\n",
    "# Drop 'building_id' (not needed for training)\n",
    "train_values.drop(columns=['building_id'], inplace=True)\n",
    "train_labels.drop(columns=['building_id'], inplace=True)\n",
    "\n",
    "# Encode target variable (damage_grade)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels['damage_grade'] = label_encoder.fit_transform(train_labels['damage_grade'])\n",
    "\n",
    "# Convert categorical columns to strings\n",
    "categorical_features = ['land_surface_condition', 'foundation_type', 'roof_type', \n",
    "                        'ground_floor_type', 'other_floor_type', 'position', \n",
    "                        'plan_configuration', 'legal_ownership_status']\n",
    "train_values[categorical_features] = train_values[categorical_features].astype(str)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_values = pd.get_dummies(train_values, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = train_values.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_values[numerical_features] = scaler.fit_transform(train_values[numerical_features])\n",
    "\n",
    "# Convert train_labels to numeric\n",
    "y = train_labels['damage_grade'].astype(np.int64)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_values, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train, X_val = X_train.to_numpy(), X_val.to_numpy()\n",
    "X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "y_train, y_val = y_train.astype(np.int64), y_val.astype(np.int64)\n",
    "\n",
    "# Reshape data for BiLSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "# Define the BiLSTM model\n",
    "def build_bilstm_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_bilstm_model((X_train.shape[1], 1))\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "checkpoint = callbacks.ModelCheckpoint(\"best_model_bilstm.keras\", monitor='val_accuracy', save_best_only=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                    epochs=100, batch_size=256, \n",
    "                    callbacks=[early_stopping, checkpoint, reduce_lr], verbose=1)\n",
    "\n",
    "# Save training history plot\n",
    "def save_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "save_training_history(history)\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions = np.argmax(model.predict(X_val), axis=1)\n",
    "cm = confusion_matrix(y_val, predictions)\n",
    "pd.DataFrame(cm).to_csv('confusion_matrix.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, predictions, target_names=label_encoder.classes_.astype(str), output_dict=True)\n",
    "pd.DataFrame(report).T.to_csv('BLSTM_model_result.csv', index=True)\n",
    "\n",
    "print(\"Results saved: training_history.png, confusion_matrix.csv, BLSTM_model_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43bdab-9f9e-4144-bbf6-e01efbbb2a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835582a6-b6ae-4f64-a264-e1fcd2065ed5",
   "metadata": {},
   "source": [
    "## GBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cb1b5-1362-411b-b753-94cf1810d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "train_values = pd.read_csv(\"/kaggle/input/earthquake-survey/train_values.csv\")\n",
    "train_labels = pd.read_csv(\"/kaggle/input/earthquake-survey/train_labels.csv\")\n",
    "\n",
    "# Drop 'building_id'\n",
    "train_values.drop(columns=['building_id'], inplace=True)\n",
    "train_labels.drop(columns=['building_id'], inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels['damage_grade'] = label_encoder.fit_transform(train_labels['damage_grade'])\n",
    "\n",
    "# Convert categorical columns to string\n",
    "categorical_features = ['land_surface_condition', 'foundation_type', 'roof_type', \n",
    "                        'ground_floor_type', 'other_floor_type', 'position', \n",
    "                        'plan_configuration', 'legal_ownership_status']\n",
    "train_values[categorical_features] = train_values[categorical_features].astype(str)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_values = pd.get_dummies(train_values, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = train_values.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_values[numerical_features] = scaler.fit_transform(train_values[numerical_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_values, train_labels['damage_grade'], test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, max_depth=8, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Extract XGBoost features (leaf indices)\n",
    "X_train_leaves = xgb_model.apply(X_train)\n",
    "X_val_leaves = xgb_model.apply(X_val)\n",
    "\n",
    "# Convert to float32\n",
    "X_train_leaves = X_train_leaves.astype(np.float32)\n",
    "X_val_leaves = X_val_leaves.astype(np.float32)\n",
    "\n",
    "# Define Neural Network Model\n",
    "def build_mlp(input_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(512, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model\n",
    "mlp_model = build_mlp(X_train_leaves.shape[1])\n",
    "history = mlp_model.fit(X_train_leaves, y_train, validation_data=(X_val_leaves, y_val),\n",
    "              epochs=100, batch_size=256, callbacks=[\n",
    "                  callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),\n",
    "                  callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "              ], verbose=1)\n",
    "\n",
    "# Save training history plot\n",
    "def save_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "save_training_history(history)\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_accuracy = mlp_model.evaluate(X_val_leaves, y_val)\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions = np.argmax(mlp_model.predict(X_val_leaves), axis=1)\n",
    "cm = confusion_matrix(y_val, predictions)\n",
    "pd.DataFrame(cm).to_csv('confusion_matrix.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, predictions, target_names=label_encoder.classes_.astype(str), output_dict=True)\n",
    "pd.DataFrame(report).T.to_csv('GBNN_model_result.csv', index=True)\n",
    "\n",
    "print(\"Results saved: training_history.png, confusion_matrix.csv, GBNN_model_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f52ee-5973-4e55-b30a-ab90776613b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369eee0e-5f9b-400d-b330-4905fab5536c",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d11be-45dd-40fe-87ee-5cde811cc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "train_values = pd.read_csv(\"/kaggle/input/survey-dataset/train_values.csv\")\n",
    "train_labels = pd.read_csv(\"/kaggle/input/survey-dataset/train_labels.csv\")\n",
    "test_values = pd.read_csv(\"/kaggle/input/survey-dataset/test_values.csv\")\n",
    "\n",
    "# Drop 'building_id'\n",
    "train_values.drop(columns=['building_id'], inplace=True)\n",
    "train_labels.drop(columns=['building_id'], inplace=True)\n",
    "test_values.drop(columns=['building_id'], inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels['damage_grade'] = label_encoder.fit_transform(train_labels['damage_grade'])\n",
    "\n",
    "# Convert categorical columns to string\n",
    "categorical_features = ['land_surface_condition', 'foundation_type', 'roof_type', \n",
    "                        'ground_floor_type', 'other_floor_type', 'position', \n",
    "                        'plan_configuration', 'legal_ownership_status']\n",
    "train_values[categorical_features] = train_values[categorical_features].astype(str)\n",
    "test_values[categorical_features] = test_values[categorical_features].astype(str)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_values = pd.get_dummies(train_values, columns=categorical_features, drop_first=True)\n",
    "test_values = pd.get_dummies(test_values, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Ensure test_values has the same columns as train_values\n",
    "missing_cols = set(train_values.columns) - set(test_values.columns)\n",
    "for col in missing_cols:\n",
    "    test_values[col] = 0\n",
    "\n",
    "# Reorder columns to match train_values\n",
    "test_values = test_values[train_values.columns]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = train_values.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_values[numerical_features] = scaler.fit_transform(train_values[numerical_features])\n",
    "test_values[numerical_features] = scaler.transform(test_values[numerical_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_values.to_numpy(), train_labels['damage_grade'].to_numpy(),\n",
    "    test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Convert to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "test_values = test_values.to_numpy().astype(np.float32)\n",
    "\n",
    "# Define TabNet model\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=8, n_a=8, n_steps=3,\n",
    "    gamma=1.3, lambda_sparse=1e-3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type=\"entmax\"\n",
    ")\n",
    "\n",
    "# Train the TabNet model\n",
    "history = tabnet_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_name=[\"valid\"],\n",
    "    eval_metric=[\"accuracy\"],\n",
    "    max_epochs=50, patience=10,\n",
    "    batch_size=256, virtual_batch_size=128,\n",
    "    num_workers=0, drop_last=False\n",
    ")\n",
    "\n",
    "# Save training history as JSON\n",
    "history_dict = {\"valid_accuracy\": tabnet_model.history[\"valid_accuracy\"]}\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "\n",
    "# Save training history plot\n",
    "def save_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history[\"valid_accuracy\"], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "save_training_history(tabnet_model.history)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred = tabnet_model.predict(X_val)\n",
    "accuracy = (y_pred == y_val).mean()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "pd.DataFrame(cm).to_csv('confusion_matrix.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, y_pred, target_names=label_encoder.classes_.astype(str), output_dict=True)\n",
    "pd.DataFrame(report).T.to_csv('TabNet_model_result.csv', index=True)\n",
    "\n",
    "# Predict on test set\n",
    "test_predictions = tabnet_model.predict(test_values)\n",
    "pd.DataFrame({'damage_grade': test_predictions}).to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Results saved: training_history.json, training_history.png, confusion_matrix.csv, TabNet_model_result.csv, test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0b7b7-9722-4299-8330-13d8e4518a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ae614-1cf1-4f46-bf37-028f9e057933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
